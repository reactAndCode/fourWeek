import { NextRequest, NextResponse } from "next/server"
import { OpenAIEmbeddings } from "@langchain/openai"
import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory"
import OpenAI from "openai"
import path from "path"
import os from "os"
import fs from "fs/promises"

interface ChatMessage {
  role: "user" | "assistant"
  content: string
}

export async function POST(request: NextRequest) {
  try {
    const { documentId, question, chatHistory } = await request.json()

    if (!documentId || !documentId.trim()) {
      return NextResponse.json(
        { error: "ë¬¸ì„œ IDê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤." },
        { status: 400 }
      )
    }

    if (!question || !question.trim()) {
      return NextResponse.json(
        { error: "ì§ˆë¬¸ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤." },
        { status: 400 }
      )
    }

    // OpenAI API í‚¤ í™•ì¸
    const apiKey = process.env.OPENAI_API_KEY
    if (!apiKey) {
      return NextResponse.json(
        { error: "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤." },
        { status: 500 }
      )
    }

    console.log("ğŸ” [ë²¡í„° ì¿¼ë¦¬] ê²€ìƒ‰ ì‹œì‘:", {
      documentId,
      ì§ˆë¬¸_ê¸¸ì´: question.length,
    })

    // 1. ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
    const tempDir = os.tmpdir()
    const vectorDir = path.join(tempDir, "vector-stores")
    const indexPath = path.join(vectorDir, `${documentId}.json`)

    const embeddings = new OpenAIEmbeddings({
      openAIApiKey: apiKey,
      modelName: "text-embedding-3-small",
    })

    let vectorStore
    try {
      // JSON íŒŒì¼ì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ë°ì´í„° ë¡œë“œ
      const fileContent = await fs.readFile(indexPath, "utf-8")
      const vectorStoreData = JSON.parse(fileContent)

      // MemoryVectorStore ì¬êµ¬ì„±
      vectorStore = new MemoryVectorStore(embeddings)
      vectorStore.memoryVectors = vectorStoreData.memoryVectors

      console.log("âœ… [ì¸ë±ìŠ¤ ë¡œë“œ] ì„±ê³µ:", { indexPath, ë²¡í„°_ìˆ˜: vectorStoreData.memoryVectors.length })
    } catch (error) {
      console.error("âŒ [ì¸ë±ìŠ¤ ë¡œë“œ] ì‹¤íŒ¨:", error)
      return NextResponse.json(
        { error: "ë¬¸ì„œ ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë‹¤ì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”." },
        { status: 404 }
      )
    }

    // 2. ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰ (ìƒìœ„ 4ê°œ)
    const similarDocs = await vectorStore.similaritySearch(question, 4)

    console.log("ğŸ“Š [ìœ ì‚¬ë„ ê²€ìƒ‰] ì™„ë£Œ:", {
      ê²€ìƒ‰ëœ_ì²­í¬_ìˆ˜: similarDocs.length,
      ì²­í¬_ê¸¸ì´ë“¤: similarDocs.map((doc) => doc.pageContent.length),
    })

    // 3. ê²€ìƒ‰ëœ ì²­í¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    const context = similarDocs.map((doc, idx) => `[ì²­í¬ ${idx + 1}]\n${doc.pageContent}`).join("\n\n")

    console.log("ğŸ“ [ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±] ì™„ë£Œ:", {
      ì´_ì»¨í…ìŠ¤íŠ¸_ê¸¸ì´: context.length,
    })

    // 4. OpenAIë¡œ ë‹µë³€ ìƒì„±
    const openai = new OpenAI({ apiKey })

    const messages: any[] = [
      {
        role: "system",
        content: `ë‹¹ì‹ ì€ ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œì˜ ì¼ë¶€ì…ë‹ˆë‹¤:

${context}

ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ ì‹œ ì–´ëŠ ì²­í¬ì—ì„œ ì •ë³´ë¥¼ ì°¾ì•˜ëŠ”ì§€ ì–¸ê¸‰í•˜ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤.`,
      },
    ]

    // ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 3ê°œë§Œ)
    if (chatHistory && Array.isArray(chatHistory)) {
      const recentHistory = chatHistory.slice(-3)
      messages.push(
        ...recentHistory.map((msg: ChatMessage) => ({
          role: msg.role,
          content: msg.content,
        }))
      )
    }

    // í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
    messages.push({
      role: "user",
      content: question,
    })

    // OpenAI API í˜¸ì¶œ
    const completion = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages,
      temperature: 0.7,
      max_tokens: 500,
    })

    const answer = completion.choices[0]?.message?.content || "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    // ì‚¬ìš©ëŸ‰ ë¡œê¹…
    console.log("ğŸ“Š [FAISS Q&A] í† í° ì‚¬ìš©ëŸ‰:", {
      ì…ë ¥_í† í°: completion.usage?.prompt_tokens || 0,
      ì¶œë ¥_í† í°: completion.usage?.completion_tokens || 0,
      ì´_í† í°: completion.usage?.total_tokens || 0,
      ì»¨í…ìŠ¤íŠ¸_ê¸¸ì´: context.length,
      ì§ˆë¬¸_ê¸¸ì´: question.length,
      ëŒ€í™”_ê¸°ë¡: chatHistory?.length || 0,
      ëª¨ë¸: "gpt-3.5-turbo",
    })

    const estimatedCost =
      ((completion.usage?.prompt_tokens || 0) * 0.5) / 1000000 +
      ((completion.usage?.completion_tokens || 0) * 1.5) / 1000000

    console.log(`ğŸ’µ ì˜ˆìƒ ë¹„ìš©: $${estimatedCost.toFixed(6)}`)

    // ì„ë² ë”© ë¹„ìš©ë„ ì¶”ê°€ (ì§ˆë¬¸ ì„ë² ë”©)
    const queryEmbeddingCost = (Math.ceil(question.length / 4) * 0.02) / 1000000
    console.log(`ğŸ’µ ì§ˆë¬¸ ì„ë² ë”© ë¹„ìš©: $${queryEmbeddingCost.toFixed(6)}`)
    console.log(`ğŸ’° ì´ ë¹„ìš©: $${(estimatedCost + queryEmbeddingCost).toFixed(6)}`)

    return NextResponse.json({
      answer,
      retrievedChunks: similarDocs.length,
    })
  } catch (error: any) {
    console.error("FAISS query error:", error)

    // OpenAI API ì—ëŸ¬ ì²˜ë¦¬
    if (error.status === 401) {
      return NextResponse.json(
        { error: "OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤." },
        { status: 500 }
      )
    }

    if (error.status === 429) {
      return NextResponse.json(
        { error: "OpenAI API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤." },
        { status: 429 }
      )
    }

    return NextResponse.json(
      { error: `ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error.message}` },
      { status: 500 }
    )
  }
}
